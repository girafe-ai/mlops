[Docs](https://lightning.ai/docs/pytorch/stable/starter/installation.html)

[Lightning Universe](https://github.com/Lightning-Universe) (Комьюнити проекты, расширения и доп возможности)

[Lightning Flash](https://github.com/Lightning-Universe/lightning-flash) (Зоопарк готовых моделей)

[Lightning Bolts](https://lightning-bolts.readthedocs.io/en/latest/) (Еще один зоопарк моделей, датасетов и супер-коллбеков)

[Train model with billions of parameters](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html)

<details markdown="1">
<summary>Не проверял, возможно мусор</summary>

- [Сахарок для S3](https://github.com/Lightning-Universe/AWS-s3_component)
- [Сахарок для Redis](https://github.com/Lightning-Universe/Redis_component)
- [Lightning GPT](https://github.com/Lightning-Universe/lightning-GPT)

</details>


# Intro
Мир обучения нейронок уже давно перешагнул за `train_loop(...)`, `optimizer.step()`, ..., `zero_grad`, ...,
и ровно как вы не дергаете Cuda из питона и не пишите полносвязанные слои руками: ровно так же не стоит писать тренировку модели руками, потому что:
- это долго
- это баговано
- каждый раз одно и то же
- множество вещей вы просто не знаете как написать (и это нормально!)
- поддерживать простыню кода в тясячи сток просто невозможно, если вы не мазохист
- процесс добавления распределенного обучения для вас станет последней ступенью ада

В общем, если вы не получаете удовлетворения от боли и страданий (и даже если да, то все равно), то призываю использовать готовые системы автоматизии тренировок.


Далее речь пойдет именно о Lightning, потому что он эффективно решает все проблемы перечисленные выше.



# Installation
[Installation Guide](https://lightning.ai/docs/pytorch/stable/starter/installation.html)

```bash
# Pip
python -m pip install lightning

# Conda
conda install lightning -c conda-forge
```

# Concepts
Строго говоря, основная конценция заключается в избавлении разработчика от необходимости писать boilerplate код.

- **Тренировка и Валидация**: Lightning автоматизирует переключение между тренировочным и валидационным режимами (а так же test и predict режимы), контролирует циклы обучения, и предоставляет полезные коллбеки.
- **GPU и TPU**: Lightning обеспечивает простое управление ресурсами, автоматизируя развертывание модели на доступные устройства (GPUs/TPUs).
- **Масштабирование**: с легкостью (реально одной строчкой) можно использовать режимы распределенного обучения (и даже инференса)
    - Встроенные стратегии:
        - [DDP](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.DDPStrategy.html#lightning.pytorch.strategies.DDPStrategy)
        - [DeepSpeed](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.DeepSpeedStrategy.html#lightning.pytorch.strategies.DeepSpeedStrategy)
        - [Fully Sharded DP](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.FSDPStrategy.html#lightning.pytorch.strategies.FSDPStrategy)
    - Интеграция со сторонними:
        - [Horovod](https://github.com/Lightning-Universe/lightning-Horovod)
        - [Hive Mind](https://github.com/Lightning-Universe/lightning-Hivemind)
        - [Fairscale](https://github.com/Lightning-Universe/lightning-Fairscale)
        - [Colossal-AI](https://github.com/Lightning-Universe/lightning-ColossalAI)
        - [Bagua](https://github.com/Lightning-Universe/lightning-Bagua)
- **Модульность и переиспользуемость**:
    - `LightningDataModule`: Объединяет в себе весь верхний уровень работы с данными (под нижним уровнем понимаются Dataset, Collator, ...). Как и модель имеет state_dict, может загружаться в чекпонинт
    - `LightningModule`: Объединяет в себе модель, оптимизатор и логику тренировочного цикла. Может быть использован для предсказаний, а также сохранен и загружен без проблем.
    - `Callbacks`: Позволяют встраивать дополнительную логику в тренировочный цикл без изменения исходного кода модели. А так же есть зоопарк коробчных и сторонних колбеков, отвечающих практически за все хуки, которые вам захочется добавить
- **Разделение логики**:
    - Research vs Engineering: Lightning позволяет разработчикам сосредотачиваться на исследовательской части работы, выделяя логику тренировочного цикла и обработку данных в отдельные абстракции.
- **Совместимость и Интеграция**:
    - `LightningModule`: наследуется от torch.nn.Module. Lightning предоставляет естественный переход от стандартного кода PyTorch, позволяя легко адаптировать существующий код.
    - Совместимость с библиотеками: Обширная поддержка различных библиотек и фреймворков для логирования, визуализации и др.
- **Безопасность и Отказоустойчивость**:
    - Остановка и Возобновление: Возможность остановки обучения и последующего возобновления с того же места.
    - Очистка: Автоматическое освобождение ресурсов GPU после тренировки.
- **Продвинутые Техники из коробки**:
    - **N-bit Precision**: Простой переход к половинной точности для увеличения скорости обучения, сокращения затрат на память и сохранения точности модели.
        - `bf16-mixed`
        - **BitSandBytes** ([BNB](https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html#quantization-via-bitsandbytes)):
            - `nf4`: Нормализованный float 4-bit тип данных
            - `nf4-dq`: "dq" расшифровывается как "Double Quantization", что позволяет уменьшить средний объем занимаемой памяти за счет квантования констант квантования. В среднем это составляет около 0.37 бит на параметр (примерно 3 ГБ для 65B модели)
            - `fp4`: Использует стандартный float 4-bit тип данных
            - `fp4-dq`: "dq" расшифровывается как "Double Quantization", что позволяет уменьшить средний объем занимаемой памяти за счет квантования констант квантования. В среднем это составляет около 0.37 бит на параметр (примерно 3 ГБ для 65B модели).
            - `int8`: Использует unsigned int8 тип данных.
            - `int8-training`: int8 для активаций и fp16 для весов.
    - **Gradient Accumulation / Clipping**: Накопление градиентов за несколько итераций перед выполнением шага оптимизации для эффективного использования ресурсов.
- **On prem clusters и Slurm**: [tutorial](https://lightning.ai/docs/pytorch/stable/clouds/cluster_intermediate_1.html)
- **Community and Contribution**: громадный и быстрорастущий комьюнити


# Loggers
- Логгер (или их список) передается напрямую в `pl.Trainer`
- В логгер можно писать через `self.log()` внутри Lightning модулей
- Так же в логгер пишут дополнительные коллбеки и плагины, которые вы передаете в `pl.Trainer`
- Некоторые из готовых логгеров:
    - [CSV Logger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.csv_logs.html#module-lightning.pytorch.loggers.csv_logs)
    - [Mlflow Looger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.mlflow.html#module-lightning.pytorch.loggers.mlflow)
    - [Tensorboard Logger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html#module-lightning.pytorch.loggers.tensorboard)
    - [Wandb Logger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb)


# Callbacks
- Коллебки передатся в `pl.Trainer` и позволяют выполнить какое-то действие в конкретный момент процесса обучения, не изменяя исходный код модели.
- Пользователь может создавать собственные коллбеки, оверрайдя методы базового `Callback` класса.
- Некоторые полезные коллбеки, которые встроены в Lightning:
    - `ModelCheckpoint`: автоматически сохраняет чекпоинты модели в процессе обучения
    - `EarlyStopping`: останавливает обучение, когда заданная метрика перестает улучшаться
    - `LearningRateMonitor`: сохраняет значения скорости обучения в логгер
    - `GradientAccumulationScheduler`: накопление градиентов на протяжении обучения
    - `BackboneFinetuning`: позволяет тонко настраивать заданные слои модели в различных фазах обучения.
    - `StochasticWeightAveraging`: реализует SWA для улучшения устойчивости модели.
    - `QuantizationAwareTraining` (выпилили в последних версиях так как часто багует, но можно копипастнуть): для обучения, учитывая квантование весов, что полезно для деплоя
    - `Pruning`: вы и сами все знаете зачем



# Plugins and Environments
[plugind](https://lightning.ai/docs/pytorch/stable/api_references.html#plugins)
[environments](https://lightning.ai/docs/pytorch/stable/api_references.html#environments)


# FAQ

## Тренировка
### Когда следует использовать аккумуляцию градиентов?
Короткий ответ: всегда.

Даже если в видеопамять помещается необходимый вам размер батча, стоит попробовать разбить его на 2/4/8/... частей и установить делитель параметров в GradientAccumulationScheduler. Вероятнее всего вы получите **ощутимый** выигрыш в скорости, так как вообще говоря шаг оптимайзера занимает достаточно много времени (это можно узнать передав `profile=True` в `pl.Trainer`).
Из минусов то, что вы получите оврехед на передачу данных с cpu на gpu.

### Какой размер для батча использовать?
Короткий ответ: степень 2, влезающая в видеопамять, но не выше 16384.

Объяснение:
- **Аллокация памяти**: GPU часто эффективнее работают с блоками памяти, размер которых является степенью двойки. Таким образом, использование таких размеров батча может привести к более оптимальной аллокации памяти и, следовательно, более эффективному использованию ресурсов.
- **Параллелизм**: Современные архитектуры GPU оптимизированы для параллельной обработки данных, и работа с данными, размер которых является степенью двойки, может улучшить параллелизм благодаря упрощению выравнивания памяти.
- **Оптимизации библиотек**: Многие низкоуровневые библиотеки и операции (например, кубические перемножения матриц), используемые в машинном обучении, оптимизированы для работы с размерами данных, являющимися степенью двойки.

<!-- 
### Где взять срез полезных рекомендаций по обучению моделек?
Самый короткий ответ: иди на кегл, открывай discussions и сортируй по most votes. Еще вот у [этого](https://www.kaggle.com/thedevastator/) чувака хорошие подборки.


- На сегодняшний день общепринятое мнение в том, что шедулер **One cycle cosine annealing** - самый оптимальный вариант
- Используйте dropout! Добавление dropout между слоями трансформеров обычно приводит к большей стабильности обучения и более робастным результатам
- Dropout также может использоваться для небольшого увеличения производительности
- Для обычных моделей машинного обучения регуляризации L1 или L2 подойдут, но для трансформеров используйте additive и dropout на hidden layers
- используя augmentation данных для валидации или смешивайте предобученные модели с вашей обученной моделью
- Усреднение весов с последних нескольких чекпоинтов модели - реально крутяк. Поэксперементируйте с усредненим только нескольких слоев. Еще через оптюну можно подобрать линейную комбинацию весов для взвешивания чекпоинтов на валидационном сете.
- Крутая и проверенная схема: при валидации или инференсе используйте набор случайных аугментаций (картинок/текстов/прочего...) для каждого семпла несколько раз, затем усредняйте предсказание модели. Это может дать существенный прирост качества
- Эксперементируйте с оптимайзерами:
    - **AdamW** - является улучшением Adam. Предотвращает экспоненциальное затухание весов модели во внешних слоях
    - **Adafactor** - характеризуется низким потреблением памяти и масштабируемостью. Может показать отличную производительность, особенно при использовании с несколькими GPU
    - **Novograd** -  По сути, еще один оптимизатор, похожий на Adam, но с улучшенными свойствами. Использовался для обучения  bert-large
    - **Ranger** - Интересный оптимизатор с хорошими результатами, но, возможно, не так известен. 
    - **Lamb** - Оптимизатор для GPU, разработанный победителями конкурсов GLUE и QQP
    - **Lookahead** - Популярный оптимизатор, который можно использовать поверх других оптимизаторов для улучшения производительности
- warmup важен (еще бы)
- Как искать оптимальный LR? Для файнтюна трансформеров начинать с 2e-5 и понижать на экспериментах. Для CNN с 1e-4. -->
